# ML_ASSIGNMENTS
# Statistical Measures
In this project, I analyzed house pricing data in Bangalore with a focus on the price per square foot metric. I began with basic exploratory data analysis (EDA) to understand the dataset and its distribution. I then applied four statistical methods—Mean and Standard Deviation, Percentile Method, Interquartile Range (IQR), and Z-Score—to detect and handle outliers, using techniques like trimming, capping, and imputation. After cleaning, I created box plots to visually compare the effectiveness of each method in removing outliers. To assess the distribution of the price per sqft column, I used histograms and measured skewness and kurtosis before and after applying transformations such as logarithmic scaling. Lastly, I explored relationships between numerical features using correlation heatmaps and scatter plots. This comprehensive analysis helped improve data quality and prepare the dataset for further modeling or decision-making.
# EDA_and_Preprocessing
The objective of this project is to design and implement a comprehensive data preprocessing pipeline to address key challenges such as missing values, outliers, inconsistent formats, and noise within a given dataset. Starting with data exploration, I examined the structure of the dataset by listing unique values for each feature, determining their lengths, performing statistical analysis, and renaming columns for better readability. During the data cleaning phase, I handled missing and inappropriate values, replaced zeroes in the "age" column with NaN, treated null values using strategies like mean, median, or mode imputation, and removed all duplicate entries. I also identified and addressed outliers to enhance data quality. For the analysis, I filtered records where age was greater than 40 and salary less than 5000, then visualized relationships using plots and counted people by their respective locations. Next, I converted categorical features into numerical ones using label encoding and one-hot encoding to make the data suitable for machine learning. Finally, I applied both StandardScaler and MinMaxScaler to normalize the feature distributions, ensuring they are well-prepared for training robust and reliable machine learning models.
# Regression
In this regression assignment, the goal is to apply and evaluate various supervised learning regression techniques using the California Housing dataset from sklearn. The dataset provides information on housing features and median home values across California. I began by loading the dataset using fetch_california_housing and converted it into a Pandas DataFrame for easier data manipulation. During preprocessing, I checked for missing values, performed feature scaling using standardization, and justified these steps to ensure uniformity across features, especially for algorithms sensitive to feature magnitude. I then implemented five regression models: Linear Regression, Decision Tree Regressor, Random Forest Regressor, Gradient Boosting Regressor, and Support Vector Regressor (SVR). Each model was explained in terms of how it works and why it might be a good fit for the dataset. For evaluation, I used Mean Squared Error (MSE), Mean Absolute Error (MAE), and R² score to compare model performance. Based on the results, I identified the best and worst-performing algorithms with proper reasoning. The entire process was documented clearly in a Jupyter Notebook, and the project was submitted via a GitHub repository for review.
# Classification Problem
The objective of this project is to apply supervised learning classification algorithms to the Breast Cancer dataset available in the sklearn library and evaluate their performance. I began by loading the dataset and preprocessing the data, which included checking for missing values and applying feature scaling using StandardScaler to ensure consistent feature ranges—particularly important for algorithms sensitive to scale like SVM and k-NN. I implemented five classification models: Logistic Regression, Decision Tree Classifier, Random Forest Classifier, Support Vector Machine (SVM), and k-Nearest Neighbors (k-NN). For each algorithm, I provided a brief explanation of how it works and its relevance to the dataset. I then evaluated the performance of these models using metrics such as accuracy, precision, recall, and F1-score, and compared the results to determine the best and worst-performing algorithms. The entire workflow was documented in a Jupyter Notebook and shared via a GitHub repository as per submission guidelines.
# 
#







